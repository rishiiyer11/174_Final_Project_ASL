{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pjdATiXZ4uT"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAeJvmkeZ2uu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "# Transformations will be added here.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     # Normalize the Data based on mean and std deviation of training data\n",
        "    transforms.Normalize((0.519, 0.4992, 0.5140), (0.2090, 0.2375, 0.2462))]\n",
        ")\n",
        "\n",
        "trainvalset = torchvision.datasets.ImageFolder(\n",
        "        root = \"./ASL_dataset/asl_alphabet_train/asl_alphabet_train\",\n",
        "        transform = transform\n",
        "    )\n",
        "\n",
        "# Train Validation Set\n",
        "train_val_split = [0.9, 0.1]\n",
        "\n",
        "# Train and Validation Set Split\n",
        "trainset, valset = torch.utils.data.random_split(trainvalset,\n",
        "                                [round(p * len(trainvalset)) for p in train_val_split],\n",
        "                                generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "testset = torchvision.datasets.ImageFolder(\n",
        "        root = \"./ASL_dataset/asl_alphabet_test/asl_alphabet_test\",\n",
        "        transform = transform\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKL5gyFtaDHL"
      },
      "source": [
        "#### Device Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjFL_EavaGOs"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJoNjTezaSM0"
      },
      "source": [
        "#### CNN Def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHBnistBaRsw"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,\n",
        "                 activation=\"relu\",\n",
        "                 dropout_rate=0.0,\n",
        "                 num_filters=[16, 32],\n",
        "                 kernel_sizes=[3, 3]):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # select activation func\n",
        "        if activation == \"relu\":\n",
        "            self.act = nn.ReLU()\n",
        "        elif activation == \"leaky_relu\":\n",
        "            self.act = nn.LeakyReLU()\n",
        "        elif activation == \"gelu\":\n",
        "            self.act = nn.GELU()\n",
        "        else:\n",
        "            raise ValueError(\"Unknown activation\", activation)\n",
        "\n",
        "        # conv stack\n",
        "        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=kernel_sizes[0])\n",
        "        self.conv2 = nn.Conv2d(num_filters[0], num_filters[1], kernel_size=kernel_sizes[1])\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # final flat size\n",
        "        example = torch.zeros(1, 3, 32, 32)\n",
        "        with torch.no_grad():\n",
        "            example = self.pool(self.act(self.conv1(example)))\n",
        "            example = self.pool(self.act(self.conv2(example)))\n",
        "            flat_size = example.numel()\n",
        "\n",
        "        self.fc1 = nn.Linear(flat_size, 128)\n",
        "\n",
        "        # 26 ASL letters total\n",
        "        self.fc2 = nn.Linear(128, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.act(self.conv1(x)))\n",
        "        x = self.pool(self.act(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_65KR2al2N"
      },
      "source": [
        "#### Hyperparamter Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkP9OUcmapBk"
      },
      "outputs": [],
      "source": [
        "# batch size modifications\n",
        "group_A = [\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.0,  \"lr\": 0.001, \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 64},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.1,  \"lr\": 0.001, \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 32},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 128},\n",
        "]\n",
        "\n",
        "# learning rate modifications\n",
        "group_B = [\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.0001, \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 64},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.0005, \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 64},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001,  \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 64},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.005,  \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 64},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.01,   \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 64},\n",
        "]\n",
        "\n",
        "# optimizer modifications\n",
        "group_C = [\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001,  \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 64},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001,  \"optimizer\": \"sgd\", \"weight_decay\": 0.0,   \"filters\": [16,32], \"batch_size\": 64},\n",
        "]\n",
        "\n",
        "hyperparameter_sets = group_A + group_B + group_C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP7AKkCxb4qt"
      },
      "source": [
        "#### run_id gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6g5-WRzb7xd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_run_id(params):\n",
        "    base = (\n",
        "        f\"a-{params['activation']}\"\n",
        "        f\"_d-{params['dropout']}\"\n",
        "        f\"_lr-{params['lr']}\"\n",
        "        f\"_opt-{params['optimizer']}\"\n",
        "        f\"_wd-{params['weight_decay']}\"\n",
        "        f\"_f-{params['filters']}\"\n",
        "        f\"_bs-{params['batch_size']}\"\n",
        "    )\n",
        "\n",
        "    return f\"{base}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzQ84iALa2R7"
      },
      "source": [
        "#### Directory for Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "851FlZTMa33B"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"experiment_runs\"):\n",
        "    os.makedirs(\"experiment_runs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt9V0TXSa6Cm"
      },
      "source": [
        "#### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5UY9VqOa7ua"
      },
      "outputs": [],
      "source": [
        "def create_optimizer(params, name, lr, weight_decay):\n",
        "    if name == \"adam\":\n",
        "        return optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    elif name == \"sgd\":\n",
        "        return optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown optimizer:\", name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2egQ1n9UbCPz"
      },
      "source": [
        "#### Training Func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MspjYR9bDY_"
      },
      "outputs": [],
      "source": [
        "def train_model(run_id, params, trainloader, validloader, epochs=15):\n",
        "\n",
        "    # record storage\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # model\n",
        "    net = Net(\n",
        "        activation=params[\"activation\"],\n",
        "        dropout_rate=params[\"dropout\"],\n",
        "        num_filters=params[\"filters\"]\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = create_optimizer(net.parameters(), params[\"optimizer\"],\n",
        "                                 params[\"lr\"], params[\"weight_decay\"])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # for analytics\n",
        "        running_loss = 0.0\n",
        "        temp_running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        net.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            temp_running_loss += loss.item()\n",
        "\n",
        "            # accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if i % 2000 == 1999:\n",
        "                print(f\"[Run {run_id}] Epoch {epoch+1}, Batch {i+1} — Loss: {temp_running_loss/2000:.3f}\")\n",
        "                temp_running_loss = 0.0\n",
        "\n",
        "        # epoch metrics\n",
        "        train_losses.append(running_loss / len(trainloader))\n",
        "        train_accs.append(100 * correct / total)\n",
        "\n",
        "        # valid loop\n",
        "        net.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for data in validloader:\n",
        "                images, labels = data[0].to(device), data[1].to(device)\n",
        "                outputs = net(images)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += predicted.eq(labels).sum().item()\n",
        "\n",
        "        valid_acc = 100 * correct_test / total_test\n",
        "        valid_accs.append(valid_acc)\n",
        "\n",
        "        print(f\"[Run {run_id}] Epoch {epoch+1} — Train Acc: {train_accs[-1]:.2f}%  Valid Acc: {valid_acc:.2f}%\")\n",
        "\n",
        "    print(\"Finished Training Run\", run_id)\n",
        "\n",
        "    # save model\n",
        "    model_path = f\"experiment_runs/model_run_{run_id}.pt\"\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    # save metrics\n",
        "    history = {\n",
        "        \"params\": params,\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\": train_accs,\n",
        "        \"valid_acc\": valid_accs\n",
        "    }\n",
        "\n",
        "    json_path = f\"experiment_runs/history_run_{run_id}.json\"\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(history, f, indent=4)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sMm8_ZqbQ8X"
      },
      "source": [
        "#### Run All"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWOpV46lbS0L"
      },
      "outputs": [],
      "source": [
        "all_histories = {}\n",
        "\n",
        "for run_id, hp in enumerate(hyperparameter_sets):\n",
        "\n",
        "    run_id = make_run_id(hp)\n",
        "\n",
        "    print(\"\\n------------------------------------\")\n",
        "    print(\"Starting Run\", run_id)\n",
        "    print(\"Hyperparameters:\", hp)\n",
        "    print(\"------------------------------------\\n\")\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=hp[\"batch_size\"],\n",
        "                                        shuffle=True, num_workers=2)\n",
        "\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=hp[\"batch_size\"],\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    history = train_model(run_id, hp, trainloader, valloader, epochs=50)\n",
        "    all_histories[run_id] = history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cur_O6JfbjKo"
      },
      "source": [
        "#### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU5e5uxtbkkk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from torchmetrics import ConfusionMatrix\n",
        "\n",
        "def load_all_histories():\n",
        "    histories = {}\n",
        "    for file in os.listdir(\"experiment_runs\"):\n",
        "        if file.startswith(\"history_run_\") and file.endswith(\".json\"):\n",
        "            run_id = int(file.split(\"_\")[2].split(\".\")[0])\n",
        "            with open(os.path.join(\"experiment_runs\", file), \"r\") as f:\n",
        "                histories[run_id] = json.load(f)\n",
        "    return histories\n",
        "\n",
        "histories = load_all_histories()\n",
        "\n",
        "# plot\n",
        "for run_id, h in histories.items():\n",
        "    plt.figure(figsize=(10,4))\n",
        "\n",
        "    # loss\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(h[\"train_loss\"])\n",
        "    plt.title(f\"Run {run_id} – Train Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "    # accuracy\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(h[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(h[\"test_acc\"], label=\"Valid Acc\")\n",
        "    plt.title(f\"Run {run_id} – Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cur_O6JfbjKo"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
