{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cur_O6JfbjKo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports\n"
      ],
      "metadata": {
        "id": "0pjdATiXZ4uT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAeJvmkeZ2uu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Device Init"
      ],
      "metadata": {
        "id": "vKL5gyFtaDHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "wjFL_EavaGOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN Def"
      ],
      "metadata": {
        "id": "kJoNjTezaSM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,\n",
        "                 activation=\"relu\",\n",
        "                 dropout_rate=0.0,\n",
        "                 num_filters=[16, 32],\n",
        "                 kernel_sizes=[3, 3]):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # select activation func\n",
        "        if activation == \"relu\":\n",
        "            self.act = nn.ReLU()\n",
        "        elif activation == \"leaky_relu\":\n",
        "            self.act = nn.LeakyReLU()\n",
        "        elif activation == \"gelu\":\n",
        "            self.act = nn.GELU()\n",
        "        else:\n",
        "            raise ValueError(\"Unknown activation\", activation)\n",
        "\n",
        "        # conv stack\n",
        "        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=kernel_sizes[0])\n",
        "        self.conv2 = nn.Conv2d(num_filters[0], num_filters[1], kernel_size=kernel_sizes[1])\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # final flat size\n",
        "        example = torch.zeros(1, 3, 32, 32)\n",
        "        with torch.no_grad():\n",
        "            example = self.pool(self.act(self.conv1(example)))\n",
        "            example = self.pool(self.act(self.conv2(example)))\n",
        "            flat_size = example.numel()\n",
        "\n",
        "        self.fc1 = nn.Linear(flat_size, 128)\n",
        "\n",
        "        # 26 ASL letters total\n",
        "        self.fc2 = nn.Linear(128, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.act(self.conv1(x)))\n",
        "        x = self.pool(self.act(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wHBnistBaRsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparamter Test Set"
      ],
      "metadata": {
        "id": "_f_65KR2al2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropout modifications\n",
        "group_A = [\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.0,  \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.1,  \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.5,  \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.75, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "]\n",
        "\n",
        "# learning rate modifications\n",
        "group_B = [\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.0001, \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.0005, \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001,  \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.005,  \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.01,   \"optimizer\": \"adam\", \"weight_decay\": 0.0,   \"filters\": [16,32]},\n",
        "]\n",
        "\n",
        "# activation function modifications\n",
        "group_C = [\n",
        "    {\"activation\": \"relu\",       \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.001, \"filters\": [32,64]},\n",
        "    {\"activation\": \"leaky_relu\", \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.001, \"filters\": [32,64]},\n",
        "    {\"activation\": \"gelu\",       \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.001, \"filters\": [32,64]},\n",
        "    {\"activation\": \"tanh\",       \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.001, \"filters\": [32,64]},\n",
        "    {\"activation\": \"sigmoid\",    \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.001, \"filters\": [32,64]},\n",
        "]\n",
        "\n",
        "# filter size modifications\n",
        "group_D = [\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0001, \"filters\": [8,16]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0001, \"filters\": [16,32]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0001, \"filters\": [32,64]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0001, \"filters\": [64,128]},\n",
        "    {\"activation\": \"relu\", \"dropout\": 0.25, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0001, \"filters\": [96,192]},\n",
        "]\n",
        "\n",
        "hyperparameter_sets = group_A + group_B + group_C + group_D\n",
        "\n"
      ],
      "metadata": {
        "id": "CkP9OUcmapBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### run_id gen"
      ],
      "metadata": {
        "id": "xP7AKkCxb4qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_run_id(params):\n",
        "    base = (\n",
        "        f\"a-{params['activation']}\"\n",
        "        f\"_d-{params['dropout']}\"\n",
        "        f\"_lr-{params['lr']}\"\n",
        "        f\"_opt-{params['optimizer']}\"\n",
        "        f\"_wd-{params['weight_decay']}\"\n",
        "        f\"_f-{params['filters']}\"\n",
        "    )\n",
        "\n",
        "    return f\"{base}\""
      ],
      "metadata": {
        "id": "k6g5-WRzb7xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Directory for Results\n"
      ],
      "metadata": {
        "id": "qzQ84iALa2R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"experiment_runs\"):\n",
        "    os.makedirs(\"experiment_runs\")"
      ],
      "metadata": {
        "id": "851FlZTMa33B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer"
      ],
      "metadata": {
        "id": "Jt9V0TXSa6Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimizer(params, name, lr, weight_decay):\n",
        "    if name == \"adam\":\n",
        "        return optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    elif name == \"sgd\":\n",
        "        return optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown optimizer:\", name)"
      ],
      "metadata": {
        "id": "s5UY9VqOa7ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Func"
      ],
      "metadata": {
        "id": "2egQ1n9UbCPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(run_id, params, trainloader, testloader, epochs=15):\n",
        "\n",
        "    # record storage\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "\n",
        "    # model\n",
        "    net = Net(\n",
        "        activation=params[\"activation\"],\n",
        "        dropout_rate=params[\"dropout\"],\n",
        "        num_filters=params[\"filters\"]\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = create_optimizer(net.parameters(), params[\"optimizer\"],\n",
        "                                 params[\"lr\"], params[\"weight_decay\"])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # for analytics\n",
        "        running_loss = 0.0\n",
        "        temp_running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        net.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            temp_running_loss += loss.item()\n",
        "\n",
        "            # accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if i % 2000 == 1999:\n",
        "                print(f\"[Run {run_id}] Epoch {epoch+1}, Batch {i+1} — Loss: {temp_running_loss/2000:.3f}\")\n",
        "                temp_running_loss = 0.0\n",
        "\n",
        "        # epoch metrics\n",
        "        train_losses.append(running_loss / len(trainloader))\n",
        "        train_accs.append(100 * correct / total)\n",
        "\n",
        "        # test loop\n",
        "        net.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                images, labels = data[0].to(device), data[1].to(device)\n",
        "                outputs = net(images)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += predicted.eq(labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct_test / total_test\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        print(f\"[Run {run_id}] Epoch {epoch+1} — Train Acc: {train_accs[-1]:.2f}%  Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    print(\"Finished Training Run\", run_id)\n",
        "\n",
        "    # save model\n",
        "    model_path = f\"experiment_runs/model_run_{run_id}.pt\"\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    # save metrics\n",
        "    history = {\n",
        "        \"params\": params,\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\": train_accs,\n",
        "        \"test_acc\": test_accs\n",
        "    }\n",
        "\n",
        "    json_path = f\"experiment_runs/history_run_{run_id}.json\"\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(history, f, indent=4)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "2MspjYR9bDY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run All"
      ],
      "metadata": {
        "id": "_sMm8_ZqbQ8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_histories = {}\n",
        "\n",
        "for run_id, hp in enumerate(hyperparameter_sets):\n",
        "\n",
        "    run_id = make_run_id(hp)\n",
        "\n",
        "    print(\"\\n------------------------------------\")\n",
        "    print(\"Starting Run\", run_id)\n",
        "    print(\"Hyperparameters:\", hp)\n",
        "    print(\"------------------------------------\\n\")\n",
        "\n",
        "    history = train_model(run_id, hp, trainloader, testloader, epochs=50)\n",
        "    all_histories[run_id] = history"
      ],
      "metadata": {
        "id": "dWOpV46lbS0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Graphs"
      ],
      "metadata": {
        "id": "cur_O6JfbjKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_all_histories():\n",
        "    histories = {}\n",
        "    for file in os.listdir(\"experiment_runs\"):\n",
        "        if file.startswith(\"history_run_\") and file.endswith(\".json\"):\n",
        "            run_id = int(file.split(\"_\")[2].split(\".\")[0])\n",
        "            with open(os.path.join(\"experiment_runs\", file), \"r\") as f:\n",
        "                histories[run_id] = json.load(f)\n",
        "    return histories\n",
        "\n",
        "histories = load_all_histories()\n",
        "\n",
        "# plot\n",
        "for run_id, h in histories.items():\n",
        "    plt.figure(figsize=(10,4))\n",
        "\n",
        "    # loss\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(h[\"train_loss\"])\n",
        "    plt.title(f\"Run {run_id} – Train Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "    # accuracy\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(h[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(h[\"test_acc\"], label=\"Test Acc\")\n",
        "    plt.title(f\"Run {run_id} – Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "TU5e5uxtbkkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}